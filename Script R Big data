install.packages("kableExtra")
install.packages("foreign")
install.packages("haven")
install.packages("Envstats")
install.packages("factoextra")
install.packages("gridExtra")
install.packages("lmtest")
install.packages("data.table")
install.packages("PerformanceAnalytics")
install.packages("pspearman")
install.packages("car")
install.packages("VIF")
install.packages("AER")
install.packages("dplyr")
install.packages("broom")
install.packages("questionr")
install.packages("FREQ")
install.packages("ggplot2")
install.packages("GGally")
install.packages("ggeffects")
install.packages("effects")
install.packages("forestmodel")
install.packages("pscl")
install.packages("MASS")
install.packages("mlogit")
install.packages("writexl")
install.packages("rJava")
install.packages("xlsxjars")
install.packages("VGAM")
install.packages("seastests")
install.packages("tsoutliers")
install.packages("MLmetrics")
install.packages("seasonal")
install.packages("BCDating")
install.packages("mFilter")
install.packages("quantmod")
install.packages("ggplot2")
install.packages("multDM")
install.packages("zoo")
install.packages("dygraphs")
install.packages("xts")
install.packages("gets")
install.packages("readxl") 
install.packages("tidyverse")
install.packages("lgarch") 
install.packages("quantmod") 
install.packages("dynlm") 
install.packages("TSA") 
install.packages("locfit") 
install.packages("tseries") 
install.packages("fBasics") 
install.packages("RColorBrewer") 
install.packages("urca") 
install.packages("corrplot") 
install.packages("FitAR") 
install.packages("multDM") 
install.packages("robustbase")
library(multDM)
library(FitAR)
library(corrplot)
library(RColorBrewer)
library(tseries)
library(TSA)
library(dynlm)
library(quantmod)
library(urca)
library(gets)
library(readxl) 
library(tidyverse)
library(lgarch) 
library(xts)
library(dygraphs)
library(zoo)
library(multDM)
library(quantmod)
library(ggplot2)
library(scales)
library(seasonal)
library(seasonal)
library(BCDating)
library(mFilter)
library(MLmetrics)
library(tidyverse)
library(lgarch)		
library(gets)
library(seastests)
library(tsoutliers)
library(StR)
library(forecaRt)
library(seasonal)
library(seasonalview)
library(oglmx) 
library(VGAM)
library(visreg)
library(xlsxjars)
library(rJava)
library(xlsx)
library(writexl)
library(mlogit)
library(MASS)
library(pscl) 
library(forestmodel)
library(effects)
library(ggeffects)
library(ggplot2)
library(broom)
library(dplyr)
library(VIF)
library(pspearman)
library(PerformanceAnalytics)
library(car)
library(EnvStats)
library(factoextra)
library(ggplot2)
library(FactoMineR)
library(gridExtra)
library(lmtest)
library(AER)
library(haven)
library(queRtionr)
library(FREQ)
library(GGally)
library(foreign)
library(fBasics)
library(readxl)
library(tidyverse)
library(lgarch)		# Gets modelling
library(gets)
library(glmnet) 	# penalized regressions
library(rbridge)	# bridge regressions
library(doParallel)
library(aTSA)

# execute foreach loops in parallel with %dopar% operator# registerDoParallel(cores = 4)	# number of parallels (if nessary)


##Importation de la base

dlbase <- read_excel("/Users/lauriannemoriceau/Documents/M2 EKAP/Big data/database_ekap (2).xlsx")
dlbase <- data.frame(dlbase) # sinon erreur pour la suite

summary(dlbase)
str(dlbase)

training_dlbase <- dlbase
data.frame(training_dlbase)

###Quali
dlbase[27:35] <- lapply(dlbase[27:35], as.factor)

library(quantmod)
cl1=ts(dlbase$cl1,end=c(2019,12),frequency=12)
par(mfrow=c(1,1))
plot(cl1)
barChart(cl1,theme=chartTheme('white'))
addMACD()

########################################################
#############Etude de la stationarité###################
########################################################

#####Tests de stationarité (Tests ADF en trois étapes) :

noms<-as.null(c())
Liste_variables_stationaires<-data.frame()
for (n in names(dlbase[,c(1:26)])){
  Statio <- ur.df(dlbase[,c(as.character(n))], type=c("trend"), selectlags = "AIC")
  if (Statio@testreg[["coefficients"]]["tt","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire" } else {j<-"NON-Stationnaire" }
  } else { Statio <- ur.df(dlbase[,c(as.character(n))], type=c("drift"), selectlags = "AIC")}
  
  if(Statio@testreg[["coefficients"]]["(Intercept)","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else { j<-"NON-Stationnaire"}
  } else{Statio <- ur.df(dlbase[,c(as.character(n))], type=c("none"), selectlags = "AIC")}
  
  if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else {j<-"NON-Stationnaire"}
  
  DF<-Statio@teststat[1,1]
  stat<-Statio@cval[1,"5pct"]
  ligne<-c(j,DF,stat)
  Liste_variables_stationaires<-rbind(Liste_variables_stationaires,ligne)
  names(Liste_variables_stationaires)<-c("Stationarité","DF","Statistic(Seuil de 5%)")
  noms<-c(noms,n)
}
row.names(Liste_variables_stationaires)<-noms
print(Liste_variables_stationaires)

# Une seule variable n'est pas stationnaire -> cu_oilgas
cu_oilgas.ts <- ts(dlbase$cu_oilgas,end=c(2019,12),frequency=12)
plot(cu_oilgas.ts)
barChart(cu_oilgas.ts,theme=chartTheme('white'))

cu_oilgas.diff.df <- ur.df(cu_oilgas.ts, type=c("none")) #on stationnarise
summary(cu_oilgas.diff.df) 

plot(cu_oilgas.diff.df)
barChart(cu_oilgas.diff.df,theme=chartTheme('white'))


##Coefficients AR1

noms<-as.null(c())
Liste_variables_stationaires<-data.frame()
for (n in names(dlbase[,c(1:26)])){
  Statio <- ur.df(dlbase[,c(as.character(n))], type=c("trend"), selectlags = "AIC")
  if (Statio@testreg[["coefficients"]]["tt","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire" } else {j<-"NON-Stationnaire" }
  } else { Statio <- ur.df(dlbase[,c(as.character(n))], type=c("drift"), selectlags = "AIC")}
  
  if(Statio@testreg[["coefficients"]]["(Intercept)","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else { j<-"NON-Stationnaire"}
  } else{Statio <- ur.df(dlbase[,c(as.character(n))], type=c("none"), selectlags = "AIC")}
  
  if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else {j<-"NON-Stationnaire"}
  
  DF<-Statio@teststat[1,1]
  stat<-Statio@cval[1,"5pct"]
  ligne<-c(j,DF,stat)
  Liste_variables_stationaires<-rbind(Liste_variables_stationaires,ligne)
  names(Liste_variables_stationaires)<-c("Stationarité","DF","Statistic(Seuil de 5%)")
  noms<-c(noms,n)
}
row.names(Liste_variables_stationaires)<-noms
print(Liste_variables_stationaires)


##TEST KPSS ? 

noms<-as.null(c())
Liste_variables_stationaires<-data.frame()
for (n in names(dlbase[,c(1:26)])){
  Statio <- kpss.test$p.value(dlbase[,c(as.character(n))])
  if (Statio@htest[["coefficients"]]["tt","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire" } else {j<-"NON-Stationnaire" }
  } else { Statio <- kpss.test(dlbase[,c(as.character(n))], type=c("drift"), selectlags = "AIC")}
  
  if(Statio@htest$p.value[["coefficients"]]["(Intercept)","Pr(>|t|)"] < 0.05){
    if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else { j<-"NON-Stationnaire"}
  } else{Statio <- kpss.test(dlbase[,c(as.character(n))], type=c("none"), selectlags = "AIC")}
  
  if(Statio@teststat[1,1] < Statio@cval[1,"5pct"]){j<-"Stationnaire"} else {j<-"NON-Stationnaire"}
  
  DF<-Statio@teststat[1,1]
  stat<-Statio@cval[1,"5pct"]
  ligne<-c(j,DF,stat)
  Liste_variables_stationaires<-rbind(Liste_variables_stationaires,ligne)
  names(Liste_variables_stationaires)<-c("Stationarité","DF","Statistic(Seuil de 5%)")
  noms<-c(noms,n)
}
row.names(Liste_variables_stationaires)<-noms
print(Liste_variables_stationaires)




########################################################
##########Statistiques descriptives#####################
########################################################


###############Analyse des outliers#####################

#Pour les variables financières (méthode de Boudt et Ali)

base_cleaned_boudt<-data.frame()
j<-0
for (n in names(dlbase[,c(1:26)])){
    y<-ts(dlbase[,c(as.character(n))])
    i<-Return.clean(y,method = "boudt")
    if(j==0){base_cleaned_boudt<-i} else{base_cleaned_boudt<-data.frame(base_cleaned_boudt,i)}
    j<-j+1
}
names(base_cleaned_boudt)<-names(dlbase[,c(1:26)])
str(dlbase)
str(base_cleaned_boudt)


# Pour les variables non-financières méthode de Chen et Liu (Ajustement Auto-ARIMA)
#-> Change toutes les valeurs après le point atypique pour suivre l'ARIMA ajusté sur toute la série
#-> Pas très bon pour des séries très volatiles 

base_cleaned_chen<-data.frame()
j<-0
for (n in names(dlbase[,c(1:26)])){
  y<-ts(dlbase[,c(as.character(n))])
  fit<-tso(y)
  i<-fit$yadj
  if(j==0){base_cleaned_chen<-i} else{base_cleaned_chen<-data.frame(base_cleaned_chen,i)}
  j<-j+1
}
names(base_cleaned_chen)<-names(dlbase[,c(1:26)])

######statistiques basiques sur base nettoyée

base_cleaned_chen2=data.frame(basicStats(base_cleaned_boudt))
base_cleaned_chen2

###sur les variables quali

quali=data.frame(basicStats(dlbase[27:34]))



#######################ACP et K-means####################

library("FactoMineR")
library("factoextra")

PCA(dlbase[2:26], scale.unit = TRUE, ncp = 5, graph = TRUE)


#####################@# Correlations ################@

library(PerformanceAnalytics)
mydata <- dlbase[1:26]
chart.Correlation(mydata, histogram=TRUE, pch=19,method = c("spearman"))


library(corrplot)
par(mfrow=c(1,1))
cor1 <- cor(dlbase[1:13],use="complete.obs",method=c("spearman"))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor1, method="color", col=col(200), 
         type="upper",
         addCoef.col = "black")

par(mfrow=c(1,1))
cor2 <- cor(dlbase[,c(1,13:26)],use="complete.obs",method=c("spearman"))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor2, method="color", col=col(200), 
         type="upper",
         addCoef.col = "black")

cor3 <- cor(dlbase[1:18],use="complete.obs",method=c("spearman"))
corrplot(cor3)
cor(dlbase[1:10])

cor4 <- cor(dlbase[,c(1,18:26)],use="complete.obs",method=c("spearman"))
corrplot(cor4)

par(mfrow=c(1,1))
cor2 <- cor(dlbase[,c(2:26)],use="complete.obs",method=c("spearman"))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor2, method="color", col=col(200), 
         type="upper",
         addCoef.col = "black")






#######################################################################
########################## Modèles de sélection #######################
#######################################################################


############################ Methode GETS #############################

# convert tibble in matrix for the function arx
class(dlbase[,2:22]) # tibble
mX = data.matrix(training_dlbase[,2:22]) # Áa marche !

# ARX model with AR(1)
Mod02WTI <- arx(training_dlbase$WTI, mc = T, ar = 1, mxreg = mX[, 1:21], vcov.type = "white") # car seulement les 2 premieres variables sont signif
Mod02WTI 

# GETS modelling
getsm_WTI2 <- getsm(Mod02WTI) 
getsm_WTI2

# GETS betas
coef.arx(getsm_WTI2)

# Get the name of relevant variables
names_mX <- names(coef.arx(getsm_WTI2))
names_mX <- names_mX[-1] # on retire le ar1
names_mX


# GETS modelling without ARCH test
getsm_WTI2b <- getsm(Mod02WTI, arch.LjungB=NULL) 
getsm_WTI2b


# GETS without AR(1)
Mod02WTI <- arx(training_dlbase$WTI, mc = T, ar = NULL, mxreg = mX[, 1:21], vcov.type = "white") # car seulement les 2 premieres variables sont signif
Mod02WTI
getsm_WTI2 <- getsm(Mod02WTI) 
getsm_WTI2


# isat function
yy <- dlbase[,1]
isat(yy, sis=TRUE, iis=FALSE, plot=TRUE, t.pval=0.005)
isat(yy, sis=FALSE, iis=TRUE, plot=TRUE, t.pval=0.005)
isat(yy, sis=FALSE, iis=FALSE, tis=TRUE, plot=TRUE, t.pval=0.005)
isat(yy, sis=TRUE, iis=TRUE, tis=TRUE, plot=TRUE, t.pval=0.005)



######################## Régressions pénalisées #######################

# standardized y and x (centered and standardized)
y <- data.frame(training_dlbase) %>%
  select(cl1) %>%
  scale(center = T, scale = F) %>%
  as.matrix()

x <- data.frame(training_dlbase) %>%
  select(-cl1) %>% # on retire la variable ‡ expliquer y
  as.matrix()

#---------------------------------------------------------------------
# Ridge regression
# 10-folds CV to choose lambda
# seq(-3, 5, length.out = 100) : random sequence of 100 numbers betwwene -3 and 5
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

# alpha = 0, implementation of ridge regression
# choix du meilleur lambda parmi 100
ridge_cv <- cv.glmnet(x, y, alpha = 0, lambda = lambdas_to_try, standardize = T, nfolds = 10) 

# Figures of lambdas
plot(ridge_cv)

# Best lambda obtained from CV (lambda.min) - other possibility: lambda.1se
lambda_cv <- ridge_cv$lambda.min

# Evaluation of the final model with the selected lambda
model_cv <- glmnet(x, y, alpha = 0, lambda = lambda_cv, standardize = T)
summary(model_cv)

# Ridge betas
model_cv$beta

#---------------------------------------------------------------------
# LASSO regression
# 10-folds CV to choose lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)


# alpha = 1, implementation of Lasso regression
lasso_cv <- cv.glmnet(x, y, alpha = 1, lambda = lambdas_to_try, standardize = T, nfolds = 10) # choix du meilleur lambda parmi 100

# Figures of lambdas
plot(lasso_cv)

# Best lambda obtained from CV (lambda.1se) - other possibility: lambda.min
lambda_cv <- lasso_cv$lambda.1se

# Evaluation of the final model with the selected lambda
model_cv <- glmnet(x, y, alpha = 1, lambda = lambda_cv, standardize = T)

# Lasso betas
model_cv$beta

# Get the name of relevant variables
which(! coef(model_cv) == 0, arr.ind = TRUE)

#---------------------------------------------------------------------
# Elastic-Net regression
# with 0 < alpha < 1
# Chhose alpha sequencially
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <- glmnet::cv.glmnet(x, y,  alpha = i, lambda = lambdas_to_try, standardize = T, nfolds = 10)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}

# Figures of lambdas
plot(cv)

# Implementation of EN regression
elasticnet_cv <- search[search$cvm == min(search$cvm), ]

# Best lambda obtained from CV (lambda.1se) - other possibility: lambda.min
lambda_cv <- elasticnet_cv$lambda.1se

# Evaluation of the final model with the selected lambda
model_cv <- glmnet(x, y, lambda = elasticnet_cv$lambda.1se, alpha = elasticnet_cv$alpha)

# EN betas
model_cv$beta

# Get the name of relevant variables
which(! coef(model_cv) == 0, arr.ind = TRUE)


#---------------------------------------------------------------------
# Adaptive Lasso regression

#LASSO
model_cv <- glmnet(x, y, alpha = 1, lambda = lambda_cv, standardize = T)
coef_lasso<-predict(model_cv,type="coef",s=lambda_cv)

# Weighted with gamma = 0.5
gamma=0.5
w0<-1/(abs(coef_lasso) + (1/length(y)))
poids.lasso<-w0^(gamma)

# Adaptive LASSO
fit_adalasso <- glmnet(x, y, penalty.factor =poids.lasso)
fit_cv_adalasso<-cv.glmnet(x, y,penalty.factor=poids.lasso)

# Figure of lambdas 
plot(fit_cv_adalasso)

# Best lambda obtained from CV (lambda.1se) - other possibility: lambda.min
lambda_cv <- fit_cv_adalasso$lambda.1se

# Evaluation of the final model with the selected lambda
model_cv <- glmnet(x, y, alpha = 1, lambda = lambda_cv, standardize = T)

# Lasso betas
model_cv$beta

# Get the name of relevant variables
which(! coef(model_cv) == 0, arr.ind = TRUE)


#---------------------------------------------------------------------
# Bridge regression
# 10-folds CV to choose lambda
# seq(-3, 5, length.out = 100) : random sequence of 100 numbers betwwene -3 and 5
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

# choix du meilleur lambda parmi 100
bridge_cv <- cv.bridge(x, y, q=0.5, lambda = lambdas_to_try, nfolds = 10) 

# Figures of lambdas
plot(bridge_cv)

# Best lambda obtained from CV (lambda.min) - other possibility: lambda.1se
lambda_cv <- bridge_cv$lambda.min

# Evaluation of the final model with the selected lambda
model_cv <- bridge(x, y, q=0.5, lambda = lambda_cv)
summary(model_cv)

# Ridge betas
model_cv$beta


#-----------------------------------------------------------------------
# Weighted fusion Lasso regressions

# Initialization of parameters
gamma=0.5
mu=0.1

# Compute pxQ matrix.
cor.mat <- cor(x)
abs.cor.mat <- abs(cor.mat)
sign.mat <- sign(cor.mat) - diag(2, nrow(cor.mat))
Wmat <- (abs.cor.mat^gamma - 1 * (abs.cor.mat == 1))/(1 -abs.cor.mat * (abs.cor.mat != 1))
weight.vec <- apply(Wmat, 1, sum)
fusion.penalty <- -sign.mat * (Wmat + diag(weight.vec))

# Compute Cholesky decomposition
R<-chol(fusion.penalty, pivot = TRUE)

# Transform Weighted Fusion in a Lasso issue sur les donn¥ees augment¥ees (1.40).
p<-dim(x)[2]
xstar<-rbind(x,sqrt(mu)*R)
ystar<-c(y,rep(0,p))

# Apply Lasso .
fit_wfusion<-glmnet(xstar,ystar)
fit_cv_wfusion<-cv.glmnet(xstar,ystar)

par(mfrow=c(1,2))
plot(fit_wfusion,xvar ="lambda",label = FALSE,col=T,xlab=expression(log(lambda)))
plot(fit_cv_wfusion,xlab=expression(log(lambda)))

min(fit_cv_wfusion$cvm)
lambda.opt<-fit_cv_wfusion$lambda.min

# We obtain the estimator of beta_weighted-fusion
fit_coef_wfusion<-predict(fit_wfusion,type="coef",s=lambda.opt)
show(fit_coef_wfusion)



##################### Régressions linéaires MCO #######################

